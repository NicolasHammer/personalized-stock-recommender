{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Personalized Stock Recommender Systems"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from src import mf_bpr, als, word2vec, metrics, datasets, utils"
   ]
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dummy Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_dummy():\n",
    "    dummy_data = pd.read_csv(\"data/dummy.data\", sep='\\t', names = ['user_id', 'item_id',      \n",
    "        'rating', 'timestamp'], engine = 'python')\n",
    "    num_users = dummy_data.user_id.unique().shape[0]\n",
    "    num_items = dummy_data.item_id.unique().shape[0]\n",
    "    return dummy_data, num_users, num_items"
   ]
  },
  {
   "source": [
    "## Matrix Factorization with BPR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Dummy Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "def train_test_dummy_bpr(dummy_data : pd.DataFrame, num_users : int, num_items : int):\n",
    "    train_items, test_items, train_list = {}, {}, []\n",
    "\n",
    "    # Iterate through every line in the raw data\n",
    "    for line in dummy_data.itertuples():\n",
    "        u, i, rating, time = line[1], line[2], line[3], line[4]\n",
    "        train_items.setdefault(u, []).append((u, i, rating, time))\n",
    "        if u not in test_items or test_items[u][2] < time:\n",
    "            test_items[u] = (i, rating, time)\n",
    "        \n",
    "    # Iterate through every user and add their samples, sorted by timestamp, to the train \n",
    "    # list\n",
    "    for u in range(1, num_users + 1):\n",
    "        train_list.extend(sorted(train_items[u], key = (lambda x : x[3])))\n",
    "\n",
    "    test_data = [(key, *value) for key, value in test_items.items()]\n",
    "\n",
    "    train_data = [item for item in train_list if item not in test_data]\n",
    "    train_data = pd.DataFrame(train_data)\n",
    "    test_data = pd.DataFrame(test_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user and item indices (zero based) and scores \n",
    "def load_dummy_bpr(dummy, num_users, num_items):\n",
    "    users, items, scores = [], [], []\n",
    "    interactions = {}\n",
    "    for line in dummy.itertuples():\n",
    "        user_index, item_index = int(line[1] - 1), int(line[2] - 1)\n",
    "        score = 1 # implicit\n",
    "\n",
    "        users.append(user_index)\n",
    "        items.append(item_index)\n",
    "        scores.append(score)\n",
    "\n",
    "        interactions.setdefault(user_index, []).append(item_index)\n",
    "\n",
    "    return users, items, scores, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "def evaluate_ranking_bpr(net, test_input, interactions, num_users, num_items):\n",
    "    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []\n",
    "    all_items = set([i for i in range(num_items)])\n",
    "    for u in range(num_users):\n",
    "        neg_items = list(all_items - set(interactions[u]))\n",
    "        user_ids, item_ids, scores = [], [], []\n",
    "        [item_ids.append(i) for i in neg_items]\n",
    "        [user_ids.append(u) for _ in neg_items]\n",
    "        test_dataset = data.TensorDataset(torch.from_numpy(np.array(user_ids)),    \n",
    "            torch.from_numpy(np.array(item_ids)))\n",
    "        test_data_iter = data.DataLoader(test_dataset, shuffle=False, batch_size=1024)\n",
    "\n",
    "        for _, (user_idxs, item_idxs) in enumerate(test_data_iter):\n",
    "            scores.extend(list(net(user_idxs, item_idxs).detach().numpy()))\n",
    "        item_scores = list(zip(item_ids, scores))\n",
    "\n",
    "        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)\n",
    "        ranked_items[u] = [r[0] for r in ranked_list[u]]\n",
    "        \n",
    "        temp = metrics.hit_and_auc(ranked_items[u], test_input[u][0], 50)\n",
    "        hit_rate.append(temp[0])\n",
    "        auc.append(temp[1])\n",
    "    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready dummy data\n",
    "dummy_data, num_users, num_items = read_dummy()\n",
    "train_dummy, test_dummy = train_test_dummy_bpr(dummy_data, num_users, num_items)\n",
    "\n",
    "# Training data\n",
    "train_users, train_items, train_ratings, interactions = load_dummy_bpr(train_dummy,    \n",
    "    num_users, num_items)\n",
    "train_dummy_dataset = datasets.DummyDataset(np.array(train_users), np.array(train_items),\n",
    "    interactions, num_items)\n",
    "train_dataloader = data.DataLoader(dataset = train_dummy_dataset, batch_size = 1024, \n",
    "    shuffle = True, num_workers = 4)\n",
    "\n",
    "# Test data\n",
    "_, _, _, test_interactions = load_dummy_bpr(test_dummy, \n",
    "    num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize model\n",
    "lr, num_epochs, wd, latent_factors = 0.01, 20, 1e-5, 10\n",
    "\n",
    "bpr_net = mf_bpr.MF_BPR(num_users, num_items, latent_factors) \n",
    "loss = mf_bpr.BPR_Loss\n",
    "optimizer = optim.Adam(bpr_net.parameters(), lr = 0.01, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "hit_rate_list = []\n",
    "auc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    accumulator, l = utils.Accumulator(2), 0.\n",
    "\n",
    "    # Train each batch\n",
    "    bpr_net.train()\n",
    "    for i, (user_idxs, item_idxs, neg_items) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        p_pos = bpr_net(user_idxs, item_idxs)\n",
    "        p_neg = bpr_net(user_idxs, neg_items)\n",
    "\n",
    "        total_loss = loss(p_pos, p_neg)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        accumulator.add(total_loss, user_idxs.shape[0])\n",
    "\n",
    "    # Evaluate\n",
    "    bpr_net.eval()\n",
    "    hit_rate, auc = evaluate_ranking_bpr(bpr_net, test_interactions, interactions, num_users,   \n",
    "        num_items)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\\n\\tloss = {accumulator[0]/accumulator[1]}\\n\\thit_rate = {hit_rate}\\n\\tauc = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "x = list(range(1, num_epochs + 1))\n",
    "plt.scatter(x, auc_list, label = \"AUC\")\n",
    "plt.scatter(x, hit_rate_list, label = \"Hit Rate\")\n",
    "plt.title(\"HR and AUC over Epoch of MF\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xticks(x[0::2])\n",
    "plt.ylim((0, 1))"
   ]
  },
  {
   "source": [
    "## Alternating Least Squares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dummy Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "def train_test_dummy_als(dummy_data : pd.DataFrame, num_users : int, num_items : int):\n",
    "    train_items, test_items, train_list = {}, {}, []\n",
    "\n",
    "    # Iterate through every line in the raw data\n",
    "    for line in dummy_data.itertuples():\n",
    "        u, i, rating, time = line[1], line[2], line[3], line[4]\n",
    "        train_items.setdefault(u, []).append((u, i, rating, time))\n",
    "        if u not in test_items or test_items[u][2] < time:\n",
    "            test_items[u] = (i, rating, time)\n",
    "        \n",
    "    # Iterate through every user and add their samples, sorted by timestamp, to the train \n",
    "    # list\n",
    "    for u in range(1, num_users + 1):\n",
    "        train_list.extend(sorted(train_items[u], key = (lambda x : x[3])))\n",
    "\n",
    "    test_data = [(key, *value) for key, value in test_items.items()]\n",
    "\n",
    "    train_data = [item for item in train_list if item not in test_data]\n",
    "    train_data = pd.DataFrame(train_data)\n",
    "    test_data = pd.DataFrame(test_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user and item indices (zero based) and scores \n",
    "def load_dummy_als(dummy, num_users, num_items):\n",
    "    users, items, scores = [], [], []\n",
    "    interactions = {}\n",
    "    for line in dummy.itertuples():\n",
    "        user_index, item_index = int(line[1] - 1), int(line[2] - 1)\n",
    "        score = 1 # implicit\n",
    "\n",
    "        users.append(user_index)\n",
    "        items.append(item_index)\n",
    "        scores.append(score)\n",
    "\n",
    "        interactions.setdefault(user_index, []).append(item_index)\n",
    "\n",
    "    return users, items, scores, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "def evaluate_ranking_als(net, test_input, interactions, num_users, num_items):\n",
    "    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []\n",
    "    all_items = set([i for i in range(num_items)])\n",
    "    for u in range(num_users):\n",
    "        neg_items = list(all_items - set(interactions[u]))\n",
    "        user_ids, item_ids, scores = [], [], []\n",
    "        [item_ids.append(i) for i in neg_items]\n",
    "        [user_ids.append(u) for _ in neg_items]\n",
    "\n",
    "        scores.extend(list(net.predict(user_ids, item_ids)))\n",
    "        item_scores = list(zip(item_ids, scores))\n",
    "\n",
    "        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)\n",
    "        ranked_items[u] = [r[0] for r in ranked_list[u]]\n",
    "        \n",
    "        temp = metrics.hit_and_auc(ranked_items[u], test_input[u][0], 50)\n",
    "        hit_rate.append(temp[0])\n",
    "        auc.append(temp[1])\n",
    "    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready dummy data\n",
    "dummy_data, num_users, num_items = read_dummy()\n",
    "train_dummy, test_dummy = train_test_dummy_als(dummy_data, num_users, num_items)\n",
    "\n",
    "# Training data\n",
    "train_users, train_items, train_ratings, interactions = load_dummy_als(train_dummy,    \n",
    "    num_users, num_items)\n",
    "\n",
    "# Test data\n",
    "_, _, _, test_interactions = load_dummy_als(test_dummy, \n",
    "    num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "num_epochs, reg, latent_factors = 20, 0.01, 30\n",
    "\n",
    "ratings_matrix = coo_matrix((train_ratings, (train_users, train_items)), shape = (num_users, \n",
    "    num_items)).todense()\n",
    "loss = mean_squared_error\n",
    "als_net = als.ALS(num_users, num_items, latent_factors, ratings_matrix, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "hit_rate_list = []\n",
    "auc_list = []\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    # Train with entire batch\n",
    "    als_net.train()\n",
    "\n",
    "    # Evaluate\n",
    "    hit_rate, auc = evaluate_ranking_als(als_net, test_interactions, interactions, num_users,\n",
    "        num_items)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    print(f\"Epoch {epoch}: hit_rate = {hit_rate}, auc = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "x = list(range(1, num_epochs + 1))\n",
    "plt.scatter(x, auc_list, label = \"AUC\")\n",
    "plt.scatter(x, hit_rate_list, label = \"Hit Rate\")\n",
    "plt.title(\"HR and AUC over Epoch of ALS\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xticks(x[0::2])\n",
    "plt.ylim((0, 1))"
   ]
  },
  {
   "source": [
    "## Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dummy Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep interactions\n",
    "def load_interactions_cbow(dummy_data : pd.DataFrame):\n",
    "    interactions = {}\n",
    "    for line in dummy_data.itertuples():\n",
    "        user_index, item_index, time = line[1] - 1, line[2] - 1, line[4]\n",
    "        interactions.setdefault(user_index, []).append((item_index, time))\n",
    "\n",
    "    interactions = {k : sorted(v, key = (lambda pair : pair[1])) for k, v in interactions.items()}\n",
    "    return {k : [x[0] for x in v] for k, v in interactions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "def train_test_dummy_cbow(interactions : dict, window : int):\n",
    "    train_targets, train_contexts = [], []\n",
    "    test_targets, test_contexts = [], []\n",
    "\n",
    "    # Iterate through every interaction\n",
    "    for user_interactions in interactions.values():\n",
    "        num_interactions = len(user_interactions)\n",
    "        # Add to training data\n",
    "        for i in range(window, num_interactions - 1):\n",
    "            train_targets.append(user_interactions[i])\n",
    "            train_contexts.append([user_interactions[j] for j in np.arange(i - window, i)])\n",
    "        # Add to testing data\n",
    "        test_targets.append(user_interactions[num_interactions - 1])\n",
    "        test_contexts.append([user_interactions[j] for j \n",
    "            in np.arange(num_interactions - 1 - window, num_interactions - 1)])\n",
    "        \n",
    "    return train_targets, train_contexts, test_targets, test_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "def evaluate_ranking_cbow(net, test_targets, test_contexts, num_items):\n",
    "    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []\n",
    "    item_ids = list(range(num_items))\n",
    "    \n",
    "    for _, (targets, contexts) in enumerate(ngrams_dataloader_test):\n",
    "        scores = net(contexts).tolist()\n",
    "        for u, row in enumerate(scores):\n",
    "            item_scores = list(zip(item_ids, row))\n",
    "            ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)\n",
    "            ranked_items[u] = [r[0] for r in ranked_list[u]]\n",
    "        \n",
    "            temp = metrics.hit_and_auc(ranked_items[u], test_targets[u], 50)\n",
    "            hit_rate.append(temp[0])\n",
    "            auc.append(temp[1])\n",
    "    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "window = 10\n",
    "\n",
    "dummy_data, num_users, num_items = read_dummy()\n",
    "sorted_interactions = load_interactions_cbow(dummy_data)\n",
    "train_targets, train_contexts, test_targets, test_contexts = train_test_dummy_cbow(sorted_interactions, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and model\n",
    "ngrams_train = data.TensorDataset(torch.from_numpy(np.array(train_targets)), \n",
    "        torch.from_numpy(np.array(train_contexts)))\n",
    "ngrams_dataloader = data.DataLoader(dataset = ngrams_train, batch_size = 1024, \n",
    "    shuffle = True, num_workers = 4)\n",
    "ngrams_test = data.TensorDataset(torch.from_numpy(np.array(test_targets)), \n",
    "    torch.from_numpy(np.array(test_contexts)))\n",
    "ngrams_dataloader_test = data.DataLoader(dataset = ngrams_test, batch_size = 1024, \n",
    "    shuffle = False, num_workers = 4)\n",
    "\n",
    "embedding_dim, num_epochs, learning_rate = 30, 20, 0.025\n",
    "loss = torch.nn.NLLLoss()\n",
    "cbow_net = word2vec.CBOW(num_items, embedding_dim, window)\n",
    "optimizer = optim.Adam(cbow_net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "hit_rate_list = []\n",
    "auc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    accumulator, l = utils.Accumulator(2), 0.\n",
    "\n",
    "    # Train each batch\n",
    "    cbow_net.train()\n",
    "    for _, (targets, contexts) in enumerate(ngrams_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_probabilities = cbow_net(contexts)\n",
    "\n",
    "        total_loss = loss(log_probabilities, targets)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        accumulator.add(total_loss, targets.shape[0])\n",
    "\n",
    "    # Evaluate\n",
    "    cbow_net.eval()\n",
    "    hit_rate, auc = evaluate_ranking_cbow(cbow_net, test_targets, test_contexts, num_items)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\\n\\tloss = {accumulator[0]/accumulator[1]}\\n\\thit_rate = {hit_rate}\\n\\tauc = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "x = list(range(1, num_epochs + 1))\n",
    "plt.scatter(x, auc_list, label = \"AUC\")\n",
    "plt.scatter(x, hit_rate_list, label = \"Hit Rate\")\n",
    "plt.title(\"HR and AUC over Epoch of CBOW\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xticks(x[0::2])\n",
    "plt.ylim((0, 1))"
   ]
  },
  {
   "source": [
    "## Visualize the Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}